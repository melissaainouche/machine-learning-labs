{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID Lab02. Naïve Bayes\n",
    "\n",
    "<p style='text-align: right;font-style: italic;'>Designed by: Mr. Abdelkrime Aries</p>\n",
    "\n",
    "In this lab, we will learn about Naive Bayes by testing 2 implementations:\n",
    "- Multinomial Naïve Bayes\n",
    "- Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "- **Member 01**: ALISMAIL Dyna Hayem\n",
    "- **Member 02**: AINOUCHE Melissa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, timeit\n",
    "from typing          import Tuple, List, Type\n",
    "from collections.abc import Callable\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2', '2.2.3', '3.9.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes   import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics       import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.metrics                 import precision_score, recall_score\n",
    "import timeit\n",
    "\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Algorithms implementation\n",
    "\n",
    "In this section, we will try to implement multinomial Naive Bayes.\n",
    "\n",
    "\n",
    "**>> Try to use \"numpy\" which will save a lot of time and effort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset play \n",
    "\n",
    "# outlook & temperature & humidity & windy\n",
    "Xplay = np.array([\n",
    "    ['sunny'   , 'hot' , 'high'  , 'no'],\n",
    "    ['sunny'   , 'hot' , 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'high'  , 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'yes'],\n",
    "    ['overcast', 'cool', 'normal', 'yes'],\n",
    "    ['sunny'   , 'mild', 'high'  , 'no'],\n",
    "    ['sunny'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'normal', 'no'],\n",
    "    ['sunny'   , 'mild', 'normal', 'yes'],\n",
    "    ['overcast', 'mild', 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'yes']\n",
    "])\n",
    "\n",
    "Yplay = np.array([\n",
    "    'no', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no'\n",
    "])\n",
    "\n",
    "len(Xplay), len(Yplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# height & weight & footsize & person\n",
    "Xperson = np.array([\n",
    "    [182., 81.6, 30.],\n",
    "    [180., 86.2, 28.],\n",
    "    [170., 77.1, 30.],\n",
    "    [180., 74.8, 25.],\n",
    "    [152., 45.4, 15.],\n",
    "    [168., 68.0, 20.],\n",
    "    [165., 59.0, 18.],\n",
    "    [175., 68.0, 23.]\n",
    "])\n",
    "\n",
    "Yperson = np.array([\n",
    "    'male', 'male', 'male', 'male',\n",
    "    'female', 'female', 'female', 'female'\n",
    "])\n",
    "\n",
    "len(Xperson), len(Yperson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Prior statistics\n",
    "\n",
    "Given an output list $Y[M]$, the probability of each class $c$ is estimated as:\n",
    "$$p(c) = \\frac{\\#(Y = c)}{|Y|}$$\n",
    "\n",
    "In here, we want to store the frequencies of different classes.\n",
    "Our function must return two lists:\n",
    "- One containing the names of unique classes.\n",
    "- Another containing their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n",
       " (array(['female', 'male'], dtype='<U6'), array([4, 4])))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Prior statistics\n",
    "def fit_prior(Y: np.ndarray[str]) -> Tuple[np.ndarray[str], np.ndarray[int]]:\n",
    "    c, f = np.unique(Y, return_counts=True)\n",
    "    return c, f\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# ((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n",
    "#  (array(['female', 'male'], dtype='<U6'), array([4, 4])))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "fit_prior(Yplay), fit_prior(Yperson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Multinomial Law\n",
    "\n",
    "In this section, we will implement multinomial naive Bayes from scratch using Numpy.\n",
    "\n",
    "#### I.2.1. Multinomial Likelihood statistics\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "\n",
    "The function takes as argument $A, Y, C$ previously described.\n",
    "It must return:\n",
    "- $V$: unique values of this feature (feature's categories)\n",
    "- $S[|C|, |V|]$: a matrix containing count $\\#(Y = c \\wedge A = v),\\, \\forall c \\in C, \\forall v \\in A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
       "  array([[0, 2, 3],\n",
       "         [4, 3, 2]])),\n",
       " (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
       "  array([[1, 2, 2],\n",
       "         [3, 2, 4]])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Multinomial Likelihood statistics\n",
    "def fit_multinomial_likelihood(A: np.ndarray[str], \n",
    "                               Y: np.ndarray[str], \n",
    "                               C: np.ndarray[str]\n",
    "                               ) -> Tuple[np.ndarray[str], np.ndarray[int]]:\n",
    "    V = np.unique(A)\n",
    "    S = np.zeros((len(C), len(V)), dtype=int)\n",
    "    \n",
    "    # Populate the count matrix S\n",
    "    for i, c in enumerate(C):\n",
    "        for j, v in enumerate(V):\n",
    "            S[i, j] = np.sum((Y == c) & (A == v))\n",
    "    \n",
    "    return V, S\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# ((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
    "#   array([[0, 2, 3],\n",
    "#          [4, 3, 2]])),\n",
    "#  (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
    "#   array([[1, 2, 2],\n",
    "#          [3, 2, 4]])))\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['no', 'yes'])\n",
    "fit_multinomial_likelihood(Xplay[:, 0], Yplay, C_t), fit_multinomial_likelihood(Xplay[:, 1], Yplay, C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Multinomial Likelihood training\n",
    "\n",
    "**Nothing to code here, although you have to know how it functions for next use**\n",
    "\n",
    "This function aims to generate parameters $\\theta$. \n",
    "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
    "They are a dictionary (map) with two entries:\n",
    "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
    "- \"likelihood\": a list of dictionaries representing statistics of each feature (the same order of $X$ features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n",
       " 'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
       "   'freq': array([[0, 2, 3],\n",
       "          [4, 3, 2]])},\n",
       "  {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
       "   'freq': array([[1, 2, 2],\n",
       "          [3, 2, 4]])},\n",
       "  {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
       "   'freq': array([[4, 1],\n",
       "          [3, 6]])},\n",
       "  {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
       "   'freq': array([[2, 3],\n",
       "          [6, 3]])}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_multinomial_NB(X: np.ndarray[str, str], \n",
    "                       Y: np.ndarray[str]\n",
    "                       ) -> object: \n",
    "    \n",
    "    Theta   = {'prior': {}, 'likelihood': []}\n",
    "\n",
    "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
    "\n",
    "    for j in range(X.shape[1]): \n",
    "        likelihood = {}\n",
    "        likelihood['vocab'], likelihood['freq'] = fit_multinomial_likelihood(X[:, j], Y, Theta['prior']['vocab'])\n",
    "        Theta['likelihood'].append(likelihood)\n",
    "    \n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# {'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n",
    "#  'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
    "#    'freq': array([[0, 2, 3],\n",
    "#           [4, 3, 2]])},\n",
    "#   {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
    "#    'freq': array([[1, 2, 2],\n",
    "#           [3, 2, 4]])},\n",
    "#   {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
    "#    'freq': array([[4, 1],\n",
    "#           [3, 6]])},\n",
    "#   {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
    "#    'freq': array([[2, 3],\n",
    "#           [6, 3]])}]}\n",
    "#---------------------------------------------------------------------\n",
    "Theta_play = fit_multinomial_NB(Xplay, Yplay)\n",
    "\n",
    "Theta_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. Multinomial Likelihood prediction\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $V$: unique values of this feature (feature's categories)\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "- $\\alpha$: smoothing factor\n",
    "\n",
    "Log likelihood is calculated as:\n",
    "$$ \\log p(A=v|Y=c) = \\log(\\#(Y = k \\wedge A = v) + \\alpha) - \\log(\\#(y = k) + \\alpha * |V|)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1), -1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this function in the next implimentation\n",
    "# It takes a list of unique values V and a given value v\n",
    "# It returns the position of v in V\n",
    "# If v does not exist in V, it rturns -1\n",
    "def find_idx(V: np.ndarray, v: str) -> int:\n",
    "    k = np.argwhere(V == v).flatten()\n",
    "    if len(k):\n",
    "        return k[0]\n",
    "    return -1\n",
    "\n",
    "V_t = np.array(['One', 'Two', 'Three'])\n",
    "find_idx(V_t, 'Two'), find_idx(V_t, 'Four')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Multinomial Likelihood prediction\n",
    "def predict_multinomial_NB1(v: str, \n",
    "                            j: int,\n",
    "                            Theta: object,  \n",
    "                            alpha: float = 0.\n",
    "                            ) -> np.ndarray[float]:\n",
    "    feature_likelihood = Theta['likelihood'][j]\n",
    "    V = feature_likelihood['vocab']\n",
    "    S = feature_likelihood['freq']\n",
    "    \n",
    "    v_idx = find_idx(V, v) \n",
    "    \n",
    "    P = Theta['prior']['freq']\n",
    "    \n",
    "    Result = np.zeros(S.shape[0])\n",
    "    \n",
    "    for i in range(S.shape[0]):\n",
    "        count = S[i, v_idx] if v_idx != -1 else 0\n",
    "        count += alpha\n",
    "        \n",
    "        total = P[i] + alpha * len(V)\n",
    "        \n",
    "        Result[i] = np.log(count) - np.log(total)\n",
    "    \n",
    "    return Result\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_t = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'normal', 'no']\n",
    "])\n",
    "\n",
    "predict_multinomial_NB1('rainy', 0, Theta_play, alpha=0.), \\\n",
    "    predict_multinomial_NB1('snowy', 0, Theta_play, alpha=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Normal (Gaussian) Law\n",
    "\n",
    "In this section, we will implement gaussian naive Bayes from scratch using Numpy.\n",
    "\n",
    "#### I.3.1. Gaussian Likelihood statistics\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "\n",
    "The function takes as argument $A, Y, C$ previously described.\n",
    "It must return $S[|C|, 2, N]$; a tensor having these dimensions:\n",
    "- first dimension: each element represents one class's statistics\n",
    "- second dimension: 1st element represents means; 2ns element represents variances\n",
    "- third dimension: each element represents mean/variance of the respective feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[165.        ,  60.1       ,  19.        ],\n",
       "        [ 92.66666667, 114.04      ,  11.33333333]],\n",
       "\n",
       "       [[178.        ,  79.925     ,  28.25      ],\n",
       "        [ 29.33333333,  25.47583333,   5.58333333]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gaussian Likelihood statistics\n",
    "def fit_gaussian_likelihood(X: np.ndarray[float], \n",
    "                            Y: np.ndarray[str], \n",
    "                            C: np.ndarray[str]\n",
    "                            ) -> Tuple['np.ndarray[C, 2, N](float)']: \n",
    "    Nb_C = len(C)  \n",
    "    Nb_F = X.shape[1]  \n",
    "    \n",
    "    \n",
    "    S = np.zeros((Nb_C, 2, Nb_F))\n",
    "    \n",
    "    for i, yi in enumerate(C):\n",
    "    \n",
    "        x = X[Y == yi]\n",
    "        \n",
    "        means = np.mean(x, axis=0)\n",
    "        \n",
    "        variances = np.var(x, axis=0, ddof=1)\n",
    "        \n",
    "        S[i, 0, :] = means  \n",
    "        S[i, 1, :] = variances  \n",
    "    \n",
    "    return S\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# array([[[165.        ,  60.1       ,  19.        ],\n",
    "#         [ 92.66666667, 114.04      ,  11.33333333]],\n",
    "\n",
    "#        [[178.        ,  79.925     ,  28.25      ],\n",
    "#         [ 29.33333333,  25.47583333,   5.58333333]]])\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['female', 'male'])\n",
    "fit_gaussian_likelihood(Xperson, Yperson, C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.2. Gaussian Likelihood training\n",
    "\n",
    "**Nothing to code here, although you have to know how it functions for next use**\n",
    "\n",
    "This function aims to generate parameters $\\theta$. \n",
    "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
    "They are a dictionary (map) with two entries:\n",
    "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
    "- \"likelihood\": a tensor of shape $[|C|, 2, N]$ containing likelihood statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
       "  'freq': array([4, 4])},\n",
       " 'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
       "         [ 92.66666667, 114.04      ,  11.33333333]],\n",
       " \n",
       "        [[178.        ,  79.925     ,  28.25      ],\n",
       "         [ 29.33333333,  25.47583333,   5.58333333]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_gaussian_NB(X: np.ndarray[str, str], \n",
    "                    Y: np.ndarray[str]\n",
    "                    ) -> object: \n",
    "    \n",
    "    Theta   = {'prior': {}, 'likelihood': []}\n",
    "\n",
    "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
    "    Theta['likelihood'] = fit_gaussian_likelihood(X, Y, Theta['prior']['vocab'])\n",
    "\n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# {'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
    "#   'freq': array([4, 4])},\n",
    "#  'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
    "#          [ 92.66666667, 114.04      ,  11.33333333]],\n",
    " \n",
    "#         [[178.        ,  79.925     ,  28.25      ],\n",
    "#          [ 29.33333333,  25.47583333,   5.58333333]]])}\n",
    "#---------------------------------------------------------------------\n",
    "Theta_person = fit_gaussian_NB(Xperson, Yperson)\n",
    "\n",
    "Theta_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.4. Gaussian Likelihood prediction\n",
    "\n",
    "Given:\n",
    "- $A$: a numerical feature\n",
    "- $\\mu_{Ac}$: mean of values of feature $A$ having $c$ as class\n",
    "- $\\sigma_{Ac}$: variance of values of feature $A$ having $c$ as class\n",
    "- $Y$: the output\n",
    "- $C$: the classes\n",
    "\n",
    "Log likelihood is calculated as:\n",
    "$$ \\log p(A=v|Y=c) = \\frac{-(v-\\mu_{Ac})^2}{2 \\sigma_{Ac}^2} - \\log(\\sqrt{2\\pi \\sigma_{Ac}^2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gaussian Likelihood prediction\n",
    "def predict_gaussian_NB1(v: str, \n",
    "                         j: int,\n",
    "                         Theta: object,  \n",
    "                         alpha: float = 0. # this is just added for compatibility\n",
    "                         ) -> np.ndarray[float]:\n",
    "    \n",
    "    C = Theta['prior']['vocab']\n",
    "    \n",
    "    M_V = Theta['likelihood']\n",
    "    \n",
    "    Log_likelihood = []\n",
    "\n",
    "    for i, yi in enumerate(C):\n",
    "    \n",
    "        mean = M_V[i][0][j]\n",
    "        variance = M_V[i][1][j]\n",
    "        \n",
    "        Log_p = - ((v - mean) ** 2) / (2 * variance) - np.log(np.sqrt(2 * np.pi * variance))\n",
    "        \n",
    "        Log_likelihood.append(Log_p)\n",
    "\n",
    "    return np.array(Log_likelihood)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "pp = predict_gaussian_NB1(183, 0, Theta_person)\n",
    "\n",
    "pp, np.exp(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Final prediction\n",
    "\n",
    "Our goal is to calculate approximate log probabilities of all classes given a sample:\n",
    "$$\\log P(y=c_k | \\overrightarrow{x} = \\overrightarrow{f})  \\approx \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j = x_j|y=c_k)$$\n",
    "\n",
    "This function takes:\n",
    "- $X^{(i)}$ one sample with $N$ features\n",
    "- $\\theta$ parameters (either those of multinomial or gaussian)\n",
    "- $pred_{fct}$ a function to predict one feauture (either multinomial or gaussian)\n",
    "- add_prior: if True, add prior probability\n",
    "- $\\alpha$ smoothing factor (passing it to gaussian function will do nothing)\n",
    "\n",
    "It must return a vector of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59617006, -4.95406494]),\n",
       " array([-2.56655064, -4.51223219]),\n",
       " array([-2.85774653, -4.23617476]),\n",
       " array([-10.401093  , -22.03977023]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Final prediction\n",
    "def predict_NB1(Xi    : 'np.ndarray[N]', \n",
    "                Theta: object,  \n",
    "                pred_fct: Callable,\n",
    "                add_prior: bool  = True,\n",
    "                alpha: float = 1.0\n",
    "                ) -> np.ndarray[float]:\n",
    "    Nb_C = len(Theta['prior']['vocab'])\n",
    "    \n",
    "    log_p = np.zeros(Nb_C)\n",
    "    \n",
    "    for i in range(Nb_C):\n",
    "        if add_prior:\n",
    "            prior_prob = np.log(Theta['prior']['freq'][i] / np.sum(Theta['prior']['freq']))\n",
    "            log_p[i] += prior_prob\n",
    "        \n",
    "        for j in range(len(Xi)):\n",
    "            log_likelihood = pred_fct(Xi[j], j, Theta, alpha)\n",
    "            log_p[i] += log_likelihood[i]\n",
    "    \n",
    "    return log_p\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-3.59617006, -4.95406494]), \n",
    "#  array([-2.56655064, -4.51223219]), \n",
    "#  array([-2.85774653, -4.23617476]), \n",
    "#  array([-10.401093 , -22.03977023]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_t1 = np.array(['sunny', 'hot' , 'high', 'no'])\n",
    "X_t2 = np.array([183., 59., 20.])\n",
    "\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=True, alpha=0.0), \\\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=0.0), \\\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=1.0), \\\n",
    "predict_NB1(X_t2, Theta_person, predict_gaussian_NB1, add_prior=False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Final product\n",
    "\n",
    "**>> Nothing to code here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['yes', 'yes', 'no'], dtype='<U3'),\n",
       " array([[-5.20912179, -4.10264337],\n",
       "        [-6.30773408, -5.48893773],\n",
       "        [-3.88736595, -4.67800751]]),\n",
       " array(['female', 'male'], dtype='<U6'),\n",
       " array([[-11.09424018, -22.73291741],\n",
       "        [-15.27968966, -12.41764665]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveBayes(object): \n",
    "\n",
    "    def __init__(self, multinomial=True):\n",
    "        if multinomial:\n",
    "            self.train = fit_multinomial_NB\n",
    "            self.pred = predict_multinomial_NB1\n",
    "        else:\n",
    "            self.train = fit_gaussian_NB\n",
    "            self.pred = predict_gaussian_NB1\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.Theta = self.train(X, Y)\n",
    "    \n",
    "    def predict(self, X, add_prior=True, prob=False, alpha=0.): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            Y_pred.append(predict_NB1(\n",
    "                X[i,:], self.Theta, self.pred, add_prior=add_prior, alpha=alpha\n",
    "                ))\n",
    "        \n",
    "        Y_pred = np.array(Y_pred)\n",
    "\n",
    "        if prob:\n",
    "            return Y_pred\n",
    "\n",
    "        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['vocab'])\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result:\n",
    "# (array(['yes', 'yes', 'no'], dtype='<U3'),\n",
    "#  array([[-5.20912179, -4.10264337],\n",
    "#         [-6.30773408, -5.48893773],\n",
    "#         [-3.88736595, -4.67800751]]),\n",
    "#  array(['female', 'male'], dtype='<U6'),\n",
    "#  array([[-11.09424018, -22.73291741],\n",
    "#         [-15.27968966, -12.41764665]]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "multinomial_nb = NaiveBayes()\n",
    "multinomial_nb.fit(Xplay, Yplay)\n",
    "\n",
    "gaussian_nb = NaiveBayes(multinomial=False)\n",
    "gaussian_nb.fit(Xperson, Yperson)\n",
    "\n",
    "X_t1 = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'high', 'no']\n",
    "])\n",
    "\n",
    "X_t2 = np.array([\n",
    "    [183., 59., 20.],\n",
    "    [175., 65., 30.]\n",
    "])\n",
    "\n",
    "\n",
    "multinomial_nb.predict(X_t1, alpha=1.), \\\n",
    "    multinomial_nb.predict(X_t1, alpha=1., prob=True), \\\n",
    "    gaussian_nb.predict(X_t2), \\\n",
    "    gaussian_nb.predict(X_t2, prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application and Analysis\n",
    "\n",
    "In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n",
    "\n",
    "### II.1. Prior probability \n",
    "\n",
    "We want to test the effect of prior probability.\n",
    "To do this, we trained two models:\n",
    "1. With prior probability\n",
    "1. Without prior probability (It considers a uniform distribution of classes)\n",
    "\n",
    "To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considring prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "No prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.67      0.80      0.73         5\n",
      "         yes       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.77      0.79      0.78        14\n",
      "weighted avg       0.80      0.79      0.79        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
    "nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n",
    "\n",
    "enc         = OrdinalEncoder()\n",
    "Xplay_tf    = enc.fit_transform(Xplay)\n",
    "nb_withPrior.fit(Xplay_tf, Yplay)\n",
    "nb_noPrior.fit(Xplay_tf, Yplay)\n",
    "\n",
    "Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n",
    "Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print( 'Considring prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_withPrior))\n",
    "\n",
    "print( 'No prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_noPrior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if prior probability is useful in this case?\n",
    "1. How does this probability affect the outcome?\n",
    "1. When are we sure that using this probability is unnecessary? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. We notice that the model with prior probability performs better across all metrics compared to the model without prior probability. Yes, in this case, the prior probability is important and useful because the classes have different prior probabilities. By using the prior, the model can account for the frequency of each class in the dataset.\n",
    "1. The use of prior probability affects the outcome by giving more weight to classes that are more frequent in the dataset. Without prior probability, the model assumes a uniform class distribution, which does not align with the actual data. By incorporating prior probability, the model better reflects the true distribution, potentially favoring more frequent classes that have a higher prior probability, which can influence the classification of new observations in their favor.\n",
    "1. We are sure that using this probability is unnecessary when the classes are balanced (equiprobable). The result of the comparison in this case between the class probabilities will not be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Smoothing\n",
    "\n",
    "We want to test the Lidstone smoothing's effect.\n",
    "To do this, we trained three models:\n",
    "1. alpha = 1 (Laplace smoothing)\n",
    "1. alpha = 0.5\n",
    "1. alpha = 0 (without smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC-PHONE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
    "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
    "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
    "\n",
    "NBC_10.fit( Xplay_tf,   Yplay )\n",
    "NBC_05.fit( Xplay_tf,   Yplay )\n",
    "NBC_00.fit( Xplay_tf,   Yplay )\n",
    "\n",
    "Y_10   = NBC_10.predict(Xplay_tf)\n",
    "Y_05   = NBC_05.predict(Xplay_tf)\n",
    "Y_00   = NBC_00.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print(                'Alpha = 1.0'                        )\n",
    "print(classification_report(Yplay, Y_10, zero_division=0.0))\n",
    "\n",
    "print(                'Alpha = 0.5'                        )\n",
    "print(classification_report(Yplay, Y_05, zero_division=0.0))\n",
    "\n",
    "print(                'Alpha = 0.0'                        )\n",
    "print(classification_report(Yplay, Y_00, zero_division=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if smoothing affects performance in this case?\n",
    "1. Based on the past answeer, Why? \n",
    "1. Why do we get a \"RuntimeWarning: divide by zero\" error? \n",
    "1. What is the benefit of smoothing (generally; not just for this case)?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. We notice that the three models have the same performance across all metrics. No, smoothing does not affect performance in this case.\n",
    "1. This may be due to the fact that the dataset is relatively small or that the features are highly predictive of the class, meaning they have a strong correlation with the class. Additionally, using the same dataset for training and testing means that there are no values to predict that do not exist in the trained model.\n",
    "1. The \"RuntimeWarning: divide by zero\" error occurs because using $\\alpha=0$ can result in zero posterior probabilities. When the logarithm is applied to these zero probabilities, it leads to undefined values, causing numerical errors. To prevent this issue, it is advisable to replace $\\alpha=0$ with a small value, ensuring that logarithmic calculations remain valid.\n",
    "1. The presence of zero posterior probabilities can cause errors during prediction because when multiplied, they nullify all other probabilities. The purpose of smoothing is to regularize probability estimates to avoid the problem of zero probabilities by adding a small amount to the posterior probabilities, ensuring that each class has at least a non-zero probability (i.e., assigning a small probability to data not encountered by the model during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. Naive Bayes performance\n",
    "\n",
    "Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n",
    "We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset.\n",
    "\n",
    "Each message is represented using term frequency (TF), where a word is considered as a feature.\n",
    "In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n",
    "We want to compare these models:\n",
    "1. Multinomial Naive Bayes (MNB)\n",
    "1. Gaussian Naive Bayes (GNB)\n",
    "1. Logistic Regression (LR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  Go until jurong point, crazy.. Available only ...   ham\n",
       "1                      Ok lar... Joking wif u oni...   ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
       "3  U dun say so early hor... U c already then say...   ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...   ham"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the dataset\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "# renaming features: text and class\n",
    "messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n",
    "# keeping only these two features\n",
    "messages = messages.filter(['text', 'class'])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Test time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes (MNB)</td>\n",
       "      <td>0.858140</td>\n",
       "      <td>0.082053</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes  (GNB)</td>\n",
       "      <td>1.014027</td>\n",
       "      <td>0.294277</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (LR)</td>\n",
       "      <td>1.508979</td>\n",
       "      <td>0.055393</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algorithm  Train time  Test time  Precision    Recall\n",
       "0  Multinomial Naive Bayes (MNB)    0.858140   0.082053   0.987179  0.927711\n",
       "1    Gaussian Naive Bayes  (GNB)    1.014027   0.294277   0.616667  0.891566\n",
       "2       Logistic Regression (LR)    1.508979   0.055393   0.986111  0.855422"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    MultinomialNB(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    #solver=sag is slower; so I chose the fastest\n",
    "]\n",
    "\n",
    "algos = [\n",
    "    'Multinomial Naive Bayes (MNB)', \n",
    "    'Gaussian Naive Bayes  (GNB)', \n",
    "    'Logistic Regression (LR)', \n",
    "]\n",
    "\n",
    "perf = {\n",
    "    'train_time': [],\n",
    "    'test_time' : [],\n",
    "    'recall'    : [],\n",
    "    'precision' : []\n",
    "}\n",
    "\n",
    "\n",
    "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n",
    "                                                        messages['class'],\n",
    "                                                        test_size    = 0.2, \n",
    "                                                        random_state = 0  )\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
    "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # ==================================\n",
    "    # TRAIN \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    model.fit(X_train, Y_train)\n",
    "    perf['train_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    Y_pred     = model.predict(X_test)\n",
    "    perf['test_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # In here, we are interrested in \"spam\" class which is our positive class\n",
    "    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
    "    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
    "\n",
    "    \n",
    "pd.DataFrame({\n",
    "    'Algorithm' : algos,\n",
    "    'Train time': perf['train_time'],\n",
    "    'Test time' : perf['test_time'],\n",
    "    'Precision' : perf['precision'],\n",
    "    'Recall'    : perf['recall']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice about training time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n",
    "1. What do you notice about the testing time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n",
    "1. How Multinomial NB's implementation affect the training/test time? (store statistics vs. store probabilities)\n",
    "1. Which one is more adequate for updating the model with new data? explain.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. We observe that the two types of Naive Bayes are the fastest in terms of training time, followed by logistic regression. The ascending order of the algorithms based on training time is as follows:\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Gaussian Naive Bayes\n",
    "    3. Logistic Regression\n",
    "1. The difference in training time for the same dataset between the algorithms depends on their respective complexities. Indeed:\n",
    "    1. **Multinomial Naive Bayes :** Is the fastest because it is particularly suited for cases where the features are discrete, such as text classification. It is simple to implement since the calculation of class and category frequencies is not complex and relies solely on counts, which simplifies the computations.\n",
    "    2. **Gaussian Naive Bayes :** The training time of this method is slower because it requires more calculations than Multinomial Naive Bayes. In addition to calculating the frequencies of each class and category, the algorithm builds a Gaussian distribution model by calculating the means and variances of the features in each class, which increases complexity.\n",
    "    3. **Logistic Regression :** is more complex than the Naive Bayes algorithms because it requires multiple iterative passes to find the optimal regression coefficients (which minimize the error rate). This iterative approach, often based on methods such as **Newton-Raphson**, involves more expensive computations, especially for datasets with a large number of features.\n",
    "1. We notice that logistic regression is the fastest in terms of testing time, followed by the two types of Naive Bayes. The ascending order of the algorithms based on testing time is as follows:\n",
    "    1. Logistic Regression\n",
    "    3. Multinomial Naive Bayes\n",
    "    4. Gaussian Naive Bayes\n",
    "1. The difference in testing time between the algorithms can be explained by how each model makes its predictions after training:\n",
    "    1. **Logistic Regression :** It depends on the number of features used for classification. For each test sample, logistic regression calculates a **linear combination** of the features and then applies a sigmoid function to produce a probability value. Since the number of features for our model is very high (each word appearing in the training dataset messages is a feature), it still manages to be efficient in terms of testing time and is the fastest.\n",
    "    2. **Multinomial Naive Bayes :** To predict the class of a new observation (message), the Multinomial Naive Bayes algorithm calculates the conditional probability for each feature (word) of each class using the frequencies already computed during training. In our case, the number of features for which we need to calculate prediction probabilities is quite high (the number of words in the texts of the test dataset). This makes the testing time a bit slower compared to logistic regression.\n",
    "    3. **Gaussian Naive Bayes :** This model is the slowest for testing due to the need to recalculate probabilities based on the Gaussian distribution. Indeed, for each prediction, it must evaluate the probability density function, which requires additional calculations to determine the mean and variance of each class for each feature. This increases the processing time compared to the other algorithms.\n",
    "1. The Gaussian model is less efficient than the multinomial model because it assumes that the data follows a **normal distribution**, which is not always the case for text data used in spam detection (thus the model will be limited by its mean and variance in the case of discrete data like word frequencies in spam messages). The multinomial model is better suited for this type of data because it models the frequency of word occurrences in messages.\n",
    "1. The Gaussian model is less efficient than the multinomial model because it is designed to work with continuous features, which is not suitable for textual data. In contrast, the multinomial model is designed to handle data with discrete features, which is appropriate for our dataset, where we use the number of occurrences of each word.\n",
    "1. The implementation of Multinomial Naive Bayes **(MNB)** affects training and testing times in two ways. By storing only raw statistics, training is faster and the model is compact, but testing takes longer since probabilities need to be computed in real-time. In contrast, by precalculating and storing the conditional probabilities **𝑃(word/class)** during training, the testing phase becomes faster, but this slows down training and requires more memory space. \n",
    "1. For updating the model with new data, storing raw statistics is more adequate. This approach allows for quicker and simpler updates because you only need to adjust the counts as new data comes in without recalculating probabilities. In contrast, if probabilities are stored, any addition of new data requires recalculating these probabilities, which is more computationally intensive and time-consuming. Thus, using raw statistics enables a more efficient process for model updates in dynamic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _____    __                                              _               \n",
      " |_   _|  / _|                                            | |              \n",
      "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
      "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
      "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
      " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
      "                   __/ |                     __/ |                         \n",
      "                  |___/                     |___/                          \n",
      "  _     _       _            __                                            \n",
      " | |   | |     (_)          / _|                 _                         \n",
      " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
      " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
      " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
      "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
      "                                                |/                         \n",
      "                                                                           \n",
      "                                                                           \n",
      "                                                                           \n",
      "  _   _    ___    _   _      __ _   _ __    ___                            \n",
      " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
      " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
      "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
      "   __/ |                                                                   \n",
      "  |___/                                                                    \n",
      "                    _                                                __    \n",
      "                   | |                                            _  \\ \\   \n",
      "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
      " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
      " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
      " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
      "                                                                     /_/   \n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"                   | |                                            _  \\ \\   \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21044\\4210918688.py:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n"
     ]
    }
   ],
   "source": [
    "print(\"  _____    __                                              _               \")\n",
    "print(\" |_   _|  / _|                                            | |              \")\n",
    "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
    "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
    "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
    "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
    "print(\"                   __/ |                     __/ |                         \")\n",
    "print(\"                  |___/                     |___/                          \")\n",
    "print(\"  _     _       _            __                                            \")\n",
    "print(\" | |   | |     (_)          / _|                 _                         \")\n",
    "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
    "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
    "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
    "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
    "print(\"                                                |/                         \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
    "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
    "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
    "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
    "print(\"   __/ |                                                                   \")\n",
    "print(\"  |___/                                                                    \")\n",
    "print(\"                    _                                                __    \")\n",
    "print(\"                   | |                                            _  \\ \\   \")\n",
    "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
    "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
    "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
    "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
    "print(\"                                                                     /_/   \")\n",
    "print(\"                                                                           \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
